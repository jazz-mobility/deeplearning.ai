{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graded Lab: Agentic Workflows\n",
    "\n",
    "In this lab, you will build an agentic system that generates a short research report through planning, external tool usage, and feedback integration. Your workflow will involve:\n",
    "\n",
    "### Agents\n",
    "\n",
    "* **Planning Agent / Writer**: Creates an outline and coordinates tasks.\n",
    "* **Research Agent**: Gathers external information using tools like Arxiv, Tavily, and Wikipedia.\n",
    "* **Editor Agent**: Reflects on the report and provides suggestions for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Tools\n",
    "\n",
    "By importing `research_tools`, you gain access to several search utilities:\n",
    "\n",
    "- `research_tools.arxiv_search_tool(query)` ‚Üí search academic papers from **arXiv**  \n",
    "\n",
    "  *Example:* `research_tools.arxiv_search_tool(\"neural networks for climate modeling\")`\n",
    "\n",
    "- `research_tools.tavily_search_tool(query)` ‚Üí perform web searches with the **Tavily API**  \n",
    "\n",
    "  *Example:* `research_tools.tavily_search_tool(\"latest trends in sunglasses fashion\")`\n",
    "\n",
    "- `research_tools.wikipedia_search_tool(query)` ‚Üí retrieve summaries from **Wikipedia**  \n",
    "\n",
    "  *Example:* `research_tools.wikipedia_search_tool(\"Ensemble Kalman Filter\")`\n",
    "\n",
    "Run the cell below to make them available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =========================\n# Imports\n# =========================\n\n# --- Standard library \nfrom datetime import datetime\nimport re\nimport json\nimport ast\n\n\n# --- Third-party ---\nfrom IPython.display import Markdown, display\nfrom aisuite import Client\n\n# --- Local / project ---\nfrom helpers import research_tools\nfrom helpers import unittests"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize client\n",
    "\n",
    "Create a shared client instance for upcoming calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: planner_agent\n",
    "\n",
    "### Objective\n",
    "Correctly set up a call to a language model (LLM) to generate a research plan.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. **Focus Areas**:\n",
    "   - Ensure `CLIENT.chat.completions.create` is correctly configured.\n",
    "   - Pass the `model` and `messages` parameters correctly:\n",
    "     - **Model**: Use `\"openai:o4-mini\"` by default.\n",
    "     - **Messages**: Set with `{\"role\": \"user\", \"content\": user_prompt}`.\n",
    "     - **Temperature**: Fixed at 1 for creative outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: planner_agent\n",
    "\n",
    "def planner_agent(topic: str, model: str = \"openai:o4-mini\") -> list[str]:\n",
    "    \"\"\"\n",
    "    Generates a plan as a Python list of steps (strings) for a research workflow.\n",
    "\n",
    "    Args:\n",
    "        topic (str): Research topic to investigate.\n",
    "        model (str): Language model to use.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of executable step strings.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Build the user prompt\n",
    "    user_prompt = f\"\"\"\n",
    "    You are a planning agent responsible for organizing a research workflow with multiple intelligent agents.\n",
    "\n",
    "    üß† Available agents:\n",
    "    - A research agent who can search the web, Wikipedia, and arXiv.\n",
    "    - A writer agent who can draft research summaries.\n",
    "    - An editor agent who can reflect and revise the drafts.\n",
    "\n",
    "    üéØ Your job is to write a clear, step-by-step research plan **as a valid Python list**, where each step is a string.\n",
    "    Each step should be atomic, executable, and must rely only on the capabilities of the above agents.\n",
    "\n",
    "    üö´ DO NOT include irrelevant tasks like \"create CSV\", \"set up a repo\", \"install packages\", etc.\n",
    "    ‚úÖ DO include real research-related tasks (e.g., search, summarize, draft, revise).\n",
    "    ‚úÖ DO assume tool use is available.\n",
    "    ‚úÖ DO NOT include explanation text ‚Äî return ONLY the Python list.\n",
    "    ‚úÖ The final step should be to generate a Markdown document containing the complete research report.\n",
    "\n",
    "    Topic: \"{topic}\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Add the user prompt to the messages list\n",
    "    messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Call the LLM\n",
    "    response = CLIENT.chat.completions.create( \n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=1, \n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Extract message from response\n",
    "    steps_str = response.choices[0].message.content.strip()\n",
    "\n",
    "    # Parse steps\n",
    "    steps = ast.literal_eval(steps_str)\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.test_planner_agent(planner_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: research_agent\n",
    "\n",
    "### Objective\n",
    "Set up a call to a language model (LLM) to perform a research task using various tools.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "**Focus Areas**:\n",
    "\n",
    "- **Creating a Custom Prompt**:\n",
    "  - **Define the Role**: Clearly specify the role, such as \"research assistant.\"\n",
    "  - **List Available Tools** (as strings inside the prompt, not the actual functions):\n",
    "    - Use `arxiv_tool` to find academic papers.\n",
    "    - Use `tavily_tool` for general web searches.\n",
    "    - Use `wikipedia_tool` for accessing encyclopedic knowledge.\n",
    "  - **Specify the Task**: Include a placeholder in your prompt for defining the specific task that needs to be accomplished.\n",
    "  - **Include Date Information**: Add a placeholder for the current date or time to provide context.\n",
    "\n",
    "- **Creating Messages Dict**:\n",
    "  - Ensure the `messages` are correctly set with `{\"role\": \"user\", \"content\": prompt}`.\n",
    "\n",
    "- **Creating Tools List**:\n",
    "  - Create a list of tools for use, such as `research_tools.arxiv_search_tool`, `research_tools.tavily_search_tool`, and `research_tools.wikipedia_search_tool`.\n",
    "\n",
    "- **Correctly Setting the Call to the LLM**:\n",
    "  - Pass the `model`, `messages`, and `tools` parameters accurately.\n",
    "  - Set `tool_choice` to `\"auto\"` for automatic tool selection.\n",
    "  - Limit interactions with `max_turns=6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: research_agent\n",
    "\n",
    "def research_agent(task: str, model: str = \"openai:gpt-4o\", return_messages: bool = False):\n",
    "    \"\"\"\n",
    "    Executes a research task using tools via aisuite (no manual loop).\n",
    "    Returns either the assistant text, or (text, messages) if return_messages=True.\n",
    "    \"\"\"\n",
    "    print(\"==================================\")  \n",
    "    print(\"üîç Research Agent\")                 \n",
    "    print(\"==================================\")\n",
    "\n",
    "    current_time = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a research assistant with access to the following tools:\n",
    "    - arxiv_tool: Use this to find academic papers on arXiv.\n",
    "    - tavily_tool: Use this for general web searches.\n",
    "    - wikipedia_tool: Use this for accessing encyclopedic knowledge.\n",
    "\n",
    "    Current date: {current_time}\n",
    "\n",
    "    Your task: {task}\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    tools = [\n",
    "        research_tools.arxiv_search_tool,\n",
    "        research_tools.tavily_search_tool,\n",
    "        research_tools.wikipedia_search_tool\n",
    "    ]\n",
    "    \n",
    "    response = CLIENT.chat.completions.create(  \n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "        max_turns=6\n",
    "    )  \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "    print(\"‚úÖ Output:\\n\", content)\n",
    "\n",
    "    \n",
    "    return (content, messages) if return_messages else content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.test_research_agent(research_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: writer_agent\n",
    "\n",
    "### Objective\n",
    "Set up a call to a language model (LLM) for executing writing tasks like drafting, expanding, or summarizing text.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. **Focus Areas**:\n",
    "   - **System Prompt**:\n",
    "     - Define `system_prompt` to assign the LLM the role of a writing agent focused on generating academic or technical content.\n",
    "   - **System and User Messages**:\n",
    "     - Create `system_msg` using `{\"role\": \"system\", \"content\": system_prompt}`.\n",
    "     - Create `user_msg` using `{\"role\": \"user\", \"content\": task}`.\n",
    "   - **Messages List**:\n",
    "     - Combine `system_msg` and `user_msg` into a `messages` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: writer_agent\n",
    "def writer_agent(task: str, model: str = \"openai:gpt-4o\") -> str:\n",
    "    \"\"\"\n",
    "    Executes writing tasks, such as drafting, expanding, or summarizing text.\n",
    "    \"\"\"\n",
    "    print(\"==================================\")\n",
    "    print(\"‚úçÔ∏è Writer Agent\")\n",
    "    print(\"==================================\")\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    system_prompt = \"You are a writing agent specialized in generating well-structured academic or technical content.\"\n",
    "\n",
    "    system_msg = {\"role\": \"system\", \"content\": system_prompt}\n",
    "\n",
    "    user_msg = {\"role\": \"user\", \"content\": task}\n",
    "\n",
    "    messages = [system_msg, user_msg]\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    response = CLIENT.chat.completions.create(\n",
    "        model=model, \n",
    "        messages=messages,\n",
    "        temperature=1.0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.test_writer_agent(writer_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: editor_agent\n",
    "\n",
    "### Objective\n",
    "Configure a call to a language model (LLM) to perform editorial tasks such as reflecting, critiquing, or revising drafts.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. **Focus Areas**:\n",
    "   - **System Prompt**:\n",
    "     - Define `system_prompt` to assign the LLM the role of an editor agent whose task is to reflect on, critique, or improve drafts.\n",
    "   - **System and User Messages**:\n",
    "     - Create `system_msg` using `{\"role\": \"system\", \"content\": system_prompt}`.\n",
    "     - Create `user_msg` using `{\"role\": \"user\", \"content\": task}`.\n",
    "   - **Messages List**:\n",
    "     - Combine `system_msg` and `user_msg` into a `messages` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: editor_agent\n",
    "def editor_agent(task: str, model: str = \"openai:gpt-4o\") -> str:\n",
    "    \"\"\"\n",
    "    Executes editorial tasks such as reflection, critique, or revision.\n",
    "    \"\"\"\n",
    "    print(\"==================================\")\n",
    "    print(\"üß† Editor Agent\")\n",
    "    print(\"==================================\")\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    system_prompt = \"You are an editor agent specialized in reflecting on, critiquing, or improving existing drafts.\"\n",
    "    \n",
    "    system_msg = {\"role\": \"system\", \"content\": system_prompt}\n",
    "    \n",
    "    user_msg = {\"role\": \"user\", \"content\": task}\n",
    "    \n",
    "    messages = [system_msg, user_msg]\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    response = CLIENT.chat.completions.create(\n",
    "        model=model, \n",
    "        messages=messages,\n",
    "        temperature=0.7 \n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.test_editor_agent(editor_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ The Executor Agent\n",
    "\n",
    "The `executor_agent` manages the workflow by executing each step of a given plan. It:\n",
    "\n",
    "1. Decides **which agent** (`research_agent`, `writer_agent`, or `editor_agent`) should handle the step.\n",
    "2. Builds context from the outputs of previous steps.\n",
    "3. Sends the enriched task to the selected agent.\n",
    "4. Collects and stores the results in a shared history.\n",
    "\n",
    "üëâ **Do not implement or modify this function.** It is already provided as the orchestration component of the multi-agent pipeline.\n",
    "\n",
    "Notice that `planner_agent` might return a long list of steps. Because of this, the maximum number of steps is set to a maximum of 4 to keep running time reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_registry = {\n",
    "    \"research_agent\": research_agent,\n",
    "    \"editor_agent\": editor_agent,\n",
    "    \"writer_agent\": writer_agent,\n",
    "}\n",
    "\n",
    "def clean_json_block(raw: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean the contents of a JSON block that may come wrapped with Markdown backticks.\n",
    "    \"\"\"\n",
    "    raw = raw.strip()\n",
    "    if raw.startswith(\"```\"):\n",
    "        raw = re.sub(r\"^```(?:json)?\\n?\", \"\", raw)\n",
    "        raw = re.sub(r\"\\n?```$\", \"\", raw)\n",
    "    return raw.strip()\n",
    "\n",
    "def executor_agent(topic, model: str = \"openai:gpt-4o\", limit_steps: bool = True):\n",
    "\n",
    "    plan_steps = planner_agent(topic)\n",
    "    max_steps = 4\n",
    "\n",
    "    if limit_steps:\n",
    "        plan_steps = plan_steps[:min(len(plan_steps), max_steps)]\n",
    "    \n",
    "    history = []\n",
    "\n",
    "    print(\"==================================\")\n",
    "    print(\"üéØ Editor Agent\")\n",
    "    print(\"==================================\")\n",
    "\n",
    "    for i, step in enumerate(plan_steps):\n",
    "\n",
    "        agent_decision_prompt = f\"\"\"\n",
    "        You are an execution manager for a multi-agent research team.\n",
    "\n",
    "        Given the following instruction, identify which agent should perform it and extract the clean task.\n",
    "\n",
    "        Return only a valid JSON object with two keys:\n",
    "        - \"agent\": one of [\"research_agent\", \"editor_agent\", \"writer_agent\"]\n",
    "        - \"task\": a string with the instruction that the agent should follow\n",
    "\n",
    "        Only respond with a valid JSON object. Do not include explanations or markdown formatting.\n",
    "\n",
    "        Instruction: \"{step}\"\n",
    "        \"\"\"\n",
    "        response = CLIENT.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": agent_decision_prompt}],\n",
    "            temperature=0,\n",
    "        )\n",
    "\n",
    "        raw_content = response.choices[0].message.content\n",
    "        cleaned_json = clean_json_block(raw_content)\n",
    "        agent_info = json.loads(cleaned_json)\n",
    "\n",
    "        agent_name = agent_info[\"agent\"]\n",
    "        task = agent_info[\"task\"]\n",
    "\n",
    "        context = \"\\n\".join([\n",
    "            f\"Step {j+1} executed by {a}:\\n{r}\" \n",
    "            for j, (s, a, r) in enumerate(history)\n",
    "        ])\n",
    "        enriched_task = f\"\"\"\n",
    "        You are {agent_name}.\n",
    "\n",
    "        Here is the context of what has been done so far:\n",
    "        {context}\n",
    "\n",
    "        Your next task is:\n",
    "        {task}\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"\\nüõ†Ô∏è Executing with agent: `{agent_name}` on task: {task}\")\n",
    "\n",
    "        if agent_name in agent_registry:\n",
    "            output = agent_registry[agent_name](enriched_task)\n",
    "            history.append((step, agent_name, output))\n",
    "        else:\n",
    "            output = f\"‚ö†Ô∏è Unknown agent: {agent_name}\"\n",
    "            history.append((step, agent_name, output))\n",
    "\n",
    "        print(f\"‚úÖ Output:\\n{output}\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to see the full workflow without limiting the number of steps. Set limit_steps to False\n",
    "# Keep in mind this could take more than 10 minutes to complete\n",
    "executor_history = executor_agent(\"The ensemble Kalman filter for time series forecasting\", limit_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = executor_history[-1][-1].strip(\"`\")  \n",
    "display(Markdown(md))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}